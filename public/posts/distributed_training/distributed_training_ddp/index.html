<!DOCTYPE html>
<html lang="en" dir="auto">

<head><script src="/adelbennaceur/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=adelbennaceur/livereload" data-no-instant defer></script><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="noindex, nofollow">
<title>How I Built My Own (tiny) Distributed Data Parallel Engine (LizarDist) | Adel Bennaceur</title>
<meta name="keywords" content="deep learning, distributed training, Distributed Data Parallel, Data Parallel, Tensor Parallelism">
<meta name="description" content="1. What is Distributed Data Parallel?
Distributed Data Parallel (DDP) is a training strategy where multiple processes (GPUs) each hold a replica of the model. Every process gets a different subset of the data (hence “data parallel”) called mini-batches, computes forward and backward passes locally, and then synchronizes gradients across processes so that the model updates stay consistent globally.
2. How does Distributed Data Parallelism work?

     
            Figure 1: Distributed Data Parallel Workflow.
        


Let’s assume we have:">
<meta name="author" content="">
<link rel="canonical" href="http://localhost:1313/adelbennaceur/posts/distributed_training/distributed_training_ddp/">
<link crossorigin="anonymous" href="/adelbennaceur/assets/css/stylesheet.93f625d739f1d6a5c6f20c146bc6a8d26b233492b34b2220c54b12fd46a04ded.css" integrity="sha256-k/Yl1znx1qXG8gwUa8ao0msjNJKzSyIgxUsS/UagTe0=" rel="preload stylesheet" as="style">
<link rel="icon" href="http://localhost:1313/adelbennaceur/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313/adelbennaceur/favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313/adelbennaceur/favicon-32x32.png">
<link rel="apple-touch-icon" href="http://localhost:1313/adelbennaceur/apple-touch-icon.png">
<link rel="mask-icon" href="http://localhost:1313/adelbennaceur/safari-pinned-tab.svg">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<link rel="alternate" hreflang="en" href="http://localhost:1313/adelbennaceur/posts/distributed_training/distributed_training_ddp/">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --code-block-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript>
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.css"
    integrity="sha384-zh0CIslj+VczCZtlzBcjt5ppRcsAmDnRem7ESsYwWwg3m/OaJ2l4x7YBZl9Kxxib" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/katex.min.js"
    integrity="sha384-Rma6DA2IPUwhNxmrB/7S3Tno0YY7sFu9WSYMCuulLhIqYSGZ2gKCJWIqhBWqMQfh"
    crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.21/dist/contrib/auto-render.min.js"
    integrity="sha384-hCXGrW6PitJEwbkoStFjeJxv+fSOOQKOPbJxSfM6G5sWZjAyWhXiTIIAmQqnlLlh" crossorigin="anonymous" onload="renderMathInElement(document.body, 
    {
              delimiters: [
                  {left: '$$', right: '$$', display: true},
                  {left: '\\[', right: '\\]', display: true},
                  {left: '$', right: '$', display: false},
                  {left: '\\(', right: '\\)', display: false}
              ]
          }
    );"></script>

</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="http://localhost:1313/adelbennaceur/" accesskey="h" title="Adel Bennaceur (Alt + H)">Adel Bennaceur</a>
            <div class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)" aria-label="Toggle theme">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </div>
        </div>
        <ul id="menu">
            <li>
                <a href="http://localhost:1313/adelbennaceur/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/adelbennaceur/posts/" title="Blog">
                    <span>Blog</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/adelbennaceur/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/adelbennaceur/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="http://localhost:1313/adelbennaceur/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="http://localhost:1313/adelbennaceur/">Home</a>&nbsp;»&nbsp;<a href="http://localhost:1313/adelbennaceur/posts/">Posts</a></div>
    <h1 class="post-title entry-hint-parent">
      How I Built My Own (tiny) Distributed Data Parallel Engine (LizarDist)
    </h1>
    <div class="post-meta"><span title='2025-06-30 00:00:00 +0000 UTC'>June 30, 2025</span>&nbsp;·&nbsp;6 min&nbsp;·&nbsp;1251 words

</div>
  </header> <div class="toc">
    <details >
        <summary accesskey="c" title="(Alt + C)">
            <span class="details">Table of Contents</span>
        </summary>

        <div class="inner"><ul>
                <li>
                    <a href="#1-what-is-distributed-data-parallel" aria-label="1. What is Distributed Data Parallel?">1. What is Distributed Data Parallel?</a></li>
                <li>
                    <a href="#2-how-does-distributed-data-parallelism-work" aria-label="2. How does Distributed Data Parallelism work?">2. How does Distributed Data Parallelism work?</a><ul>
                        <ul>
                        
                <li>
                    <a href="#21-forward-pass-local" aria-label="2.1 Forward Pass (Local)">2.1 Forward Pass (Local)</a></li>
                <li>
                    <a href="#22-backward-pass-local-gradient-calculation" aria-label="2.2 Backward Pass (Local Gradient Calculation)">2.2 Backward Pass (Local Gradient Calculation)</a></li>
                <li>
                    <a href="#23-gradient-synchronization-global-gradient-calculationaveraging" aria-label="2.3 Gradient Synchronization (Global Gradient Calculation/Averaging)">2.3 Gradient Synchronization (Global Gradient Calculation/Averaging)</a></li>
                <li>
                    <a href="#24-parameter-update-local-update-using-global-gradients" aria-label="2.4 Parameter Update (Local Update Using Global Gradients)">2.4 Parameter Update (Local Update Using Global Gradients)</a></li>
                <li>
                    <a href="#25-gradient-bucketing-reducing-communication-overhead" aria-label="2.5 Gradient Bucketing: Reducing Communication Overhead">2.5 Gradient Bucketing: Reducing Communication Overhead</a></li></ul>
                    </ul>
                </li>
                <li>
                    <a href="#3-implementation-of-ddp-in-lizardist" aria-label="3. Implementation of DDP in LizarDist">3. Implementation of DDP in LizarDist</a><ul>
                        
                <li>
                    <a href="#31-process-initialization-and-communication-backend" aria-label="3.1 Process Initialization and Communication Backend">3.1 Process Initialization and Communication Backend</a><ul>
                        
                <li>
                    <a href="#32-model-replication-and-weight-broadcast" aria-label="3.2 Model Replication and Weight Broadcast">3.2 Model Replication and Weight Broadcast</a></li>
                <li>
                    <a href="#33-manual-dataset-sharding" aria-label="3.3 Manual Dataset Sharding">3.3 Manual Dataset Sharding</a></li>
                <li>
                    <a href="#34-gradient-synchronization-the-core-of-ddp" aria-label="3.4 Gradient Synchronization: The Core of DDP">3.4 Gradient Synchronization: The Core of DDP</a></li>
                <li>
                    <a href="#35-bucketing-strategy-optimize-allreduce" aria-label="3.5 Bucketing Strategy: Optimize AllReduce">3.5 Bucketing Strategy: Optimize AllReduce</a></li>
                <li>
                    <a href="#36-optimizer-step-same-as-standard-pytorch" aria-label="3.6 Optimizer Step: Same as Standard PyTorch">3.6 Optimizer Step: Same as Standard PyTorch</a></li>
                <li>
                    <a href="#37-full-example-training-loop" aria-label="3.7 Full Example Training Loop">3.7 Full Example Training Loop</a></li>
                <li>
                    <a href="#38-pitfalls-ask-me-how-i-know" aria-label="3.8 Pitfalls (Ask Me How I Know…)">3.8 Pitfalls (Ask Me How I Know…)</a></li></ul>
                </li></ul>
                </li>
                <li>
                    <a href="#4-conclusion" aria-label="4. Conclusion">4. Conclusion</a>
                </li>
            </ul>
        </div>
    </details>
</div>

  <div class="post-content"><h1 id="1-what-is-distributed-data-parallel">1. What is Distributed Data Parallel?<a hidden class="anchor" aria-hidden="true" href="#1-what-is-distributed-data-parallel">#</a></h1>
<p>Distributed Data Parallel (DDP) is a training strategy where multiple processes (GPUs) each hold a replica of the model. Every process gets a different subset of the data (hence “data parallel”) called mini-batches, computes forward and backward passes locally, and then synchronizes gradients across processes so that the model updates stay consistent globally.</p>
<h1 id="2-how-does-distributed-data-parallelism-work">2. How does Distributed Data Parallelism work?<a hidden class="anchor" aria-hidden="true" href="#2-how-does-distributed-data-parallelism-work">#</a></h1>
<figure>
    <img loading="lazy" src="ddp_workflow.png"/> <figcaption>
            Figure 1: Distributed Data Parallel Workflow.
        </figcaption>
</figure>

<p>Let’s assume we have:</p>
<ul>
<li>$N$ processes (each on a GPU or node).</li>
<li>Each process has the same model replica.</li>
<li>Each process gets a different mini-batch.</li>
</ul>
<p>Here&rsquo;s what happens per training iteration:</p>
<h3 id="21-forward-pass-local">2.1 Forward Pass (Local)<a hidden class="anchor" aria-hidden="true" href="#21-forward-pass-local">#</a></h3>
<p>Each process computes the forward pass locally:
$$y_i  =f(x_i,\theta)$$</p>
<p>where $f$ is the forward computational graph, $x_i$ is the input (at process $i$) and $\theta$ are the local model parameters (initially identical across all process)</p>
<h3 id="22-backward-pass-local-gradient-calculation">2.2 Backward Pass (Local Gradient Calculation)<a hidden class="anchor" aria-hidden="true" href="#22-backward-pass-local-gradient-calculation">#</a></h3>
<p>Each process independently computes gradients from its local loss:
$$g_i =\nabla_\theta L_i(f(x_i,\theta), y^{target}_i)$$</p>
<h3 id="23-gradient-synchronization-global-gradient-calculationaveraging">2.3 Gradient Synchronization (Global Gradient Calculation/Averaging)<a hidden class="anchor" aria-hidden="true" href="#23-gradient-synchronization-global-gradient-calculationaveraging">#</a></h3>
<p><em>Here’s where distributed data parallelism kicks in</em>. We want to compute the global gradient as an average of the local gradients:</p>
<p>$$g_{avg} = \frac{1}{N}\sum_{i=1}^{N} g_i$$</p>
<p>This is done using the <code>AllReduce</code> operation (implemented by the backend like NCCL, GLOO, MPI, etc.) that does the following:</p>
<ol>
<li><strong>Apply a reduction</strong> (<code>sum</code> in this case) <strong>to the gradients</strong> across all processes.</li>
<li><strong>Distribute</strong> the result to all processes.</li>
</ol>
<p>A visual illustration of <code>AllReduce</code> is shown in Figure 2.
<figure><a href="#ZgotmplZ">
    <img loading="lazy" src="all_reduce.png"/> </a><figcaption>
            Figure 2: AllReduce Operation. Source: tech.preferred.jp
        </figcaption>
</figure>
</p>
<h3 id="24-parameter-update-local-update-using-global-gradients">2.4 Parameter Update (Local Update Using Global Gradients)<a hidden class="anchor" aria-hidden="true" href="#24-parameter-update-local-update-using-global-gradients">#</a></h3>
<p>Once the gradients are synchronized, each process updates the model parameters:</p>
<p>$$\theta^{(t+1)} \leftarrow \theta^{(t)} - \alpha \cdot g_{avg}$$</p>
<p>where $\theta^{(t)}$ are the model parameters at iteration $t$, $\alpha$ is the learning rate, and $g_{avg}$ is the average gradient computed in the previous step.</p>
<p>This ensures all model replicas stay synchronized.</p>
<h3 id="25-gradient-bucketing-reducing-communication-overhead">2.5 Gradient Bucketing: Reducing Communication Overhead<a hidden class="anchor" aria-hidden="true" href="#25-gradient-bucketing-reducing-communication-overhead">#</a></h3>
<p>Each tensor in our model has its own gradient. Naively, we’d call <code>AllReduce</code> per tensor&hellip;a bit inefficient. Instead, we batch them together:</p>
<ul>
<li><strong>Group gradients into larger &ldquo;buckets&rdquo;</strong></li>
<li><strong>Flatten</strong> them into a single contiguous buffer</li>
<li><strong>Call <code>AllReduce</code> once per bucket</strong>, not per tensor</li>
</ul>
<p>This reduces communication overhead significantly.</p>
<blockquote>
<p>Imagine sending letters (gradients) from one office (GPU) to another. Naively, we&rsquo;d send each letter in its own envelope (one <code>AllReduce</code> per tensor), wasting time and postage. Instead, we collect many letters, put them in one big envelope (bucket), and send it all at once (one <code>AllReduce</code> per bucket). Same contents delivered, with less overhead.</p></blockquote>
<h1 id="3-implementation-of-ddp-in-lizardist">3. Implementation of DDP in LizarDist<a hidden class="anchor" aria-hidden="true" href="#3-implementation-of-ddp-in-lizardist">#</a></h1>
<p>When I first started building LizarDist, it was just meant to be a small exercise from Jacob Hilton’s <a href="https://github.com/jacobhilton/deep_learning_curriculum">Deep Learning Curriculum</a>. LizarDist evolved into a more in depth exploration of distributed training strategies. I didn’t stop at basic data parallelism, I explored other strategies like pipeline and model parallelism that will be included in this minimal framework.</p>
<p>At its core, LizarDist is my sandbox to understand how strategies like Data Parallelism actually work. Let&rsquo;s break it down.</p>
<h2 id="31-process-initialization-and-communication-backend">3.1 Process Initialization and Communication Backend<a hidden class="anchor" aria-hidden="true" href="#31-process-initialization-and-communication-backend">#</a></h2>
<p>Each process in LizarDist is spawned using <code>mpiexec</code> and wrapped with a <code>Communicator</code> object that abstracts over the low-level MPI communication layer.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">from</span> lizardist.distributed.communicator <span style="color:#f92672">import</span> Communicator
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>comm <span style="color:#f92672">=</span> Communicator()
</span></span><span style="display:flex;"><span>rank <span style="color:#f92672">=</span> comm<span style="color:#f92672">.</span>get_rank()
</span></span><span style="display:flex;"><span>world_size <span style="color:#f92672">=</span> comm<span style="color:#f92672">.</span>get_world_size()
</span></span></code></pre></div><p>This Communicator internally holds:</p>
<ul>
<li>A reference to <code>MPI.COMM_WORLD</code></li>
<li>Rank and world size</li>
<li>Communication tracking (number of AllReduce calls, bytes sent)</li>
<li>Optional gradient bucketing logic for communication efficiency</li>
</ul>
<p>All MPI ops like <code>AllReduce</code>, <code>bcast</code>, <code>gather</code>, and <code>barrier</code> are wrapped inside this communicator for easier integration and stats tracking.</p>
<h3 id="32-model-replication-and-weight-broadcast">3.2 Model Replication and Weight Broadcast<a hidden class="anchor" aria-hidden="true" href="#32-model-replication-and-weight-broadcast">#</a></h3>
<p>Each process creates its own instance of the model:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>model <span style="color:#f92672">=</span> Model()<span style="color:#f92672">.</span>to(device)
</span></span></code></pre></div><p>But to ensure all processes start with <strong>identical weights</strong>, only <strong>rank 0</strong> initializes the parameters. We then use MPI’s broadcast mechanism to share them across all other processes:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>sync <span style="color:#f92672">=</span> Synchronizer(comm)
</span></span><span style="display:flex;"><span>sync<span style="color:#f92672">.</span>broadcast_parameters(model)
</span></span></code></pre></div><p>This uses:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>self<span style="color:#f92672">.</span>comm<span style="color:#f92672">.</span>bcast(param_np, root<span style="color:#f92672">=</span>root)
</span></span></code></pre></div><p>to send a copy of each parameter to all other ranks. This prevents silent divergence due to initialization randomness.</p>
<h3 id="33-manual-dataset-sharding">3.3 Manual Dataset Sharding<a hidden class="anchor" aria-hidden="true" href="#33-manual-dataset-sharding">#</a></h3>
<p>Unlike PyTorch’s built-in <a href="https://docs.pytorch.org/docs/stable/data.html#torch.utils.data.distributed.DistributedSampler"><code>DistributedSampler</code></a>, I implemented manual dataset sharding for transparency:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>dataset <span style="color:#f92672">=</span> datasets<span style="color:#f92672">.</span>MNIST(<span style="color:#e6db74">&#34;../data&#34;</span>, train<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, download<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>, transform<span style="color:#f92672">=</span>transform)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>shard_size <span style="color:#f92672">=</span> len(dataset) <span style="color:#f92672">//</span> world_size
</span></span><span style="display:flex;"><span>start_idx <span style="color:#f92672">=</span> rank <span style="color:#f92672">*</span> shard_size
</span></span><span style="display:flex;"><span>end_idx <span style="color:#f92672">=</span> dataset_size <span style="color:#66d9ef">if</span> rank <span style="color:#f92672">==</span> world_size <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span> <span style="color:#66d9ef">else</span> start_idx <span style="color:#f92672">+</span> shard_size
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>dataset <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>utils<span style="color:#f92672">.</span>data<span style="color:#f92672">.</span>Subset(dataset, range(start_idx, end_idx))
</span></span><span style="display:flex;"><span>train_loader <span style="color:#f92672">=</span> DataLoader(dataset, batch_size<span style="color:#f92672">=</span>batch_size, shuffle<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span></code></pre></div><p>Each process gets a <strong>non-overlapping subset</strong> of the dataset, ensuring full dataset coverage without duplication.</p>
<h3 id="34-gradient-synchronization-the-core-of-ddp">3.4 Gradient Synchronization: The Core of DDP<a hidden class="anchor" aria-hidden="true" href="#34-gradient-synchronization-the-core-of-ddp">#</a></h3>
<p>Once each process computes its local loss and calls <code>loss.backward()</code>, its <code>.grad</code> fields contain gradients with respect to local data. These need to be averaged across all processes to ensure that each optimizer step is globally consistent.</p>
<p>Here’s where the <code>Synchronizer</code> kicks in:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>sync<span style="color:#f92672">.</span>sync_gradients(model, use_bucketing<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>optimizer<span style="color:#f92672">.</span>step()
</span></span></code></pre></div><p>This does the following:</p>
<ul>
<li>Gathers all <code>.grad</code> tensors from the model</li>
<li>Detaches and converts them to NumPy arrays (for MPI compatibility)</li>
<li>Performs <code>AllReduce</code> across processes using:
<ul>
<li><strong>One call per tensor</strong> (if bucketing is off)</li>
<li><strong>Concatenated bucketed reduction</strong> (if bucketing is on)</li>
</ul>
</li>
<li>Averages the result by dividing by <code>world_size</code></li>
<li>Writes the result back to the <code>.grad</code> field using:</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>param<span style="color:#f92672">.</span>grad<span style="color:#f92672">.</span>copy_(torch<span style="color:#f92672">.</span>from_numpy(avg_grad)<span style="color:#f92672">.</span>to(param<span style="color:#f92672">.</span>device))
</span></span></code></pre></div><h3 id="35-bucketing-strategy-optimize-allreduce">3.5 Bucketing Strategy: Optimize AllReduce<a hidden class="anchor" aria-hidden="true" href="#35-bucketing-strategy-optimize-allreduce">#</a></h3>
<p>Calling AllReduce for each tensor individually incurs a lot of overhead. To reduce this, I implement gradient bucketing:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>results <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>current_bucket: list[np<span style="color:#f92672">.</span>ndarray] <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>current_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">for</span> tensor <span style="color:#f92672">in</span> tensors:
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">if</span> current_size <span style="color:#f92672">+</span> tensor<span style="color:#f92672">.</span>nbytes <span style="color:#f92672">&gt;</span> self<span style="color:#f92672">.</span>bucket_size:
</span></span><span style="display:flex;"><span>        reduced_bucket <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>_allreduce_bucket(current_bucket)
</span></span><span style="display:flex;"><span>        results<span style="color:#f92672">.</span>extend(split_into_original_shapes(reduced_bucket))
</span></span><span style="display:flex;"><span>        current_bucket <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        current_size <span style="color:#f92672">=</span> <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    current_bucket<span style="color:#f92672">.</span>append(tensor)
</span></span><span style="display:flex;"><span>    current_size <span style="color:#f92672">+=</span> tensor<span style="color:#f92672">.</span>nbytes
</span></span></code></pre></div><ul>
<li>Gradients are concatenated into a flat buffer</li>
<li>We perform one <code>AllReduce</code> on the buffer</li>
<li>We split the result and reshape each gradient back into its original shape</li>
</ul>
<p>The bucketing size is configurable (<code>bucket_size</code> in bytes) and tracked for performance diagnostics. The table below compares key communication metrics with and without bucketing for a simple CNN model:</p>
<table>
  <thead>
      <tr>
          <th>Metric</th>
          <th>With Bucketing</th>
          <th>Without Bucketing</th>
          <th>Ratio (With / Without)</th>
      </tr>
  </thead>
  <tbody>
      <tr>
          <td>Average Bytes/Call</td>
          <td>827,688</td>
          <td>103,461</td>
          <td>~8x larger</td>
      </tr>
      <tr>
          <td>Total Allreduce Calls</td>
          <td>235</td>
          <td>1880</td>
          <td>~8x fewer</td>
      </tr>
      <tr>
          <td>Total Bytes Sent</td>
          <td>194,506,680</td>
          <td>194,506,680</td>
          <td>same</td>
      </tr>
  </tbody>
</table>
<p><strong>Bucketing leads to fewer allreduce calls with larger average data per call</strong>, making communication more efficient overall.
The code for this is in the <a href="https://github.com/adelbennaceur/lizardist/blob/889833333fde4fa94444da93ef3b71edc9996813/lizardist/distributed/communicator.py#L70">Communicator Class</a>.</p>
<h3 id="36-optimizer-step-same-as-standard-pytorch">3.6 Optimizer Step: Same as Standard PyTorch<a hidden class="anchor" aria-hidden="true" href="#36-optimizer-step-same-as-standard-pytorch">#</a></h3>
<p>After gradient synchronization, each process performs a local optimizer step:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>optimizer<span style="color:#f92672">.</span>step()
</span></span></code></pre></div><p>But because gradients have already been averaged across ranks, this ensures consistent weight updates across ranks. There’s no need for additional sync.</p>
<h3 id="37-full-example-training-loop">3.7 Full Example Training Loop<a hidden class="anchor" aria-hidden="true" href="#37-full-example-training-loop">#</a></h3>
<p>Here is the key training loop:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">for</span> epoch <span style="color:#f92672">in</span> range(epochs):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">for</span> batch_idx, (data, target) <span style="color:#f92672">in</span> enumerate(train_loader):
</span></span><span style="display:flex;"><span>        data, target <span style="color:#f92672">=</span> data<span style="color:#f92672">.</span>to(device), target<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>zero_grad()
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> model(data)
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> criterion(output, target)
</span></span><span style="display:flex;"><span>        loss<span style="color:#f92672">.</span>backward()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        sync<span style="color:#f92672">.</span>sync_gradients(model, use_bucketing<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        optimizer<span style="color:#f92672">.</span>step()
</span></span></code></pre></div><p>That’s it. The rest of the training pipeline is pure PyTorch. The only change is intercepting the gradient sync step before <code>optimizer.step()</code>.</p>
<h3 id="38-pitfalls-ask-me-how-i-know">3.8 Pitfalls (Ask Me How I Know…)<a hidden class="anchor" aria-hidden="true" href="#38-pitfalls-ask-me-how-i-know">#</a></h3>
<ul>
<li>Forgetting to divide reduced gradients by world size → effective learning rate scales with number of processes → divergence.</li>
<li>Using too small bucket sizes → too many AllReduce calls → kills performance.</li>
<li>Not syncing gradients before optimizer.step() → models silently diverge → wasted compute.</li>
</ul>
<h1 id="4-conclusion">4. Conclusion<a hidden class="anchor" aria-hidden="true" href="#4-conclusion">#</a></h1>
<p>If you&rsquo;re If you&rsquo;re curious about how DDP works under the hood in more details, <a href="https://arxiv.org/abs/2006.15704">the PyTorch DDP paper</a> is a solid starting point. In parallel, I’ve been building <a href="https://github.com/adelbennaceur/lizardist">LizarDist</a>, my own minimal distributed training library. It reimplements core ideas from DDP. For those new to MPI, this <a href="https://mpitutorial.com/tutorials/">MPI tutorial</a> is a great resource. I also highly recommend Hugging Face’s <a href="https://huggingface.co/spaces/nanotron/ultrascale-playbook">Ultra-Scale Training Playbook</a> it offers practical insights into scaling strategies and real-world engineering trade-offs.</p>
<p>👉 In future posts, I’ll walk through pipeline parallelism, tensor parallelism, and hybrid strategies I&rsquo;ve been implementing in LizarDist.</p>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="http://localhost:1313/adelbennaceur/tags/deep-learning/">Deep Learning</a></li>
      <li><a href="http://localhost:1313/adelbennaceur/tags/distributed-training/">Distributed Training</a></li>
      <li><a href="http://localhost:1313/adelbennaceur/tags/distributed-data-parallel/">Distributed Data Parallel</a></li>
      <li><a href="http://localhost:1313/adelbennaceur/tags/data-parallel/">Data Parallel</a></li>
      <li><a href="http://localhost:1313/adelbennaceur/tags/tensor-parallelism/">Tensor Parallelism</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="http://localhost:1313/adelbennaceur/posts/distributed_training/distributed_training_intro/">
    <span class="title">« Prev</span>
    <br>
    <span>Exploring Distributed Deep Learning with LizardDist</span>
  </a>
  <a class="next" href="http://localhost:1313/adelbennaceur/posts/adam_vs_adamw/">
    <span class="title">Next »</span>
    <br>
    <span>Adam vs. AdamW: A Practical Deep Dive into Optimizer Differences</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
        <span>&copy; 2025 <a href="http://localhost:1313/adelbennaceur/">Adel Bennaceur</a></span> · 

    <span>
        Powered by
        <a href="https://gohugo.io/" rel="noopener noreferrer" target="_blank">Hugo</a> &
        <a href="https://github.com/adityatelange/hugo-PaperMod/" rel="noopener" target="_blank">PaperMod</a>
    </span>
</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'copy';

        function copyingDone() {
            copybutton.innerHTML = 'copied!';
            setTimeout(() => {
                copybutton.innerHTML = 'copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
