<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>Adel Bennaceur</title>
    <link>http://localhost:1313/adelbennaceur/</link>
    <description>Recent content on Adel Bennaceur</description>
    <generator>Hugo -- 0.148.2</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 30 Jun 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/adelbennaceur/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Exploring Distributed Deep Learning with LizardDist</title>
      <link>http://localhost:1313/adelbennaceur/posts/distributed_training/distributed_training_intro/</link>
      <pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/adelbennaceur/posts/distributed_training/distributed_training_intro/</guid>
      <description>&lt;p&gt;In this post, I’m laying the groundwork for my experiments with distributed training strategies using PyTorch and mpi4py. I want to explore approaches like distributed data parallelism, tensor parallelism, hybrid strategies, and more, digging into how communication, computation overlap, and scaling trade-offs work under the hood.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/adelbennaceur/lizardist&#34;&gt;LizarDist&lt;/a&gt; is the playground I’m building to test and learn these concepts. My goal is not just to build something that works, but to truly internalize the theory and practical challenges of distributed deep learning.&lt;/p&gt;</description>
    </item>
    <item>
      <title>Part 1: How I Built My Own (tiny) Distributed Data Parallel Engine (LizarDist)</title>
      <link>http://localhost:1313/adelbennaceur/posts/distributed_training/distributed_training_ddp/</link>
      <pubDate>Mon, 30 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/adelbennaceur/posts/distributed_training/distributed_training_ddp/</guid>
      <description>&lt;h1 id=&#34;1-what-is-distributed-data-parallel&#34;&gt;1. What is Distributed Data Parallel?&lt;/h1&gt;
&lt;p&gt;Distributed Data Parallel (DDP) is a training strategy where multiple processes (GPUs) each hold a replica of the model. Every process gets a different subset of the data (hence “data parallel”) called mini-batches, computes forward and backward passes locally, and then synchronizes gradients across processes so that the model updates stay consistent globally.&lt;/p&gt;
&lt;h1 id=&#34;2-how-does-distributed-data-parallelism-work&#34;&gt;2. How does Distributed Data Parallelism work?&lt;/h1&gt;
&lt;p&gt;Let’s assume we have:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Adam vs. AdamW: A Practical Deep Dive into Optimizer Differences</title>
      <link>http://localhost:1313/adelbennaceur/posts/adam_vs_adamw/</link>
      <pubDate>Fri, 04 Apr 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/adelbennaceur/posts/adam_vs_adamw/</guid>
      <description>&lt;p&gt;Hello! This is my first blog ever!!
I&amp;rsquo;ll write about Adam and AdamW. it&amp;rsquo;s always good to go back to the basics and brush up on what&amp;rsquo;s happening under the hood :), so let&amp;rsquo;s get started.&lt;/p&gt;
&lt;h1 id=&#34;background-adam-optimizer-overview&#34;&gt;Background: Adam Optimizer Overview&lt;/h1&gt;
&lt;p&gt;Adam (Adaptive Moment Estimation) is a popular stochastic optimizer introduced by &lt;a href=&#34;https://arxiv.org/abs/1412.6980&#34;&gt;Kingma and Ba (2014)&lt;/a&gt;. It combines ideas from momentum and RMSProp to adapt the learning rate for each parameter. Mathematically, Adam maintains an exponentially decaying average of past gradients (first moment) and of past squared gradients (second moment). At each step $t$, for each parameter $\theta$, Adam updates these estimates as:&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
